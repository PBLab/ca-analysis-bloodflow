from typing import List, Tuple, Union, MutableMapping, Optional
import itertools

import numpy as np
import h5py
import pandas as pd
import xarray as xr
import seaborn as sns
import attr
from attr.validators import instance_of
import sys
import pathlib
import matplotlib
import matplotlib.pyplot as plt
from matplotlib import gridspec

from calcium_bflow_analysis.analog_trace import (
    analog_trace_runner,
    AnalogAcquisitionType,
)
from calcium_bflow_analysis.fluo_metadata import FluoMetadata
import calcium_bflow_analysis.dff_analysis_and_plotting.dff_analysis as dff_tools
from calcium_bflow_analysis.dff_dataset import dff_dataset_init


@attr.s(slots=True)
class SingleFovParser:
    """ Analyze a single FOV with fluorescent and analog data """

    analog_fname = attr.ib(validator=instance_of(pathlib.Path))
    results_fname = attr.ib(validator=instance_of(pathlib.Path))
    results_hdf5 = attr.ib(validator=instance_of(pathlib.Path))
    metadata = attr.ib()  # validator=instance_of(FluoMetadata)
    analog = attr.ib(default=AnalogAcquisitionType.NONE)
    colabeled = attr.ib(default=False)
    summarize_in_plot = attr.ib(default=False, validator=instance_of(bool))
    analog_analyzed = attr.ib(init=False, repr=False)  # AnalogTraceAnalyzer instance
    all_fluo_results = attr.ib(init=False, repr=False)  # all data generated by CaImAn
    fluo_trace = attr.ib(init=False, repr=False)  # The specific dF/F trace of that file
    fluo_analyzed = attr.ib(
        init=False
    )  # xr.Dataset with the different slices of run, stim and dF/F
    fluo_colabeled = attr.ib(init=False)
    fluo_non_colabeled = attr.ib(init=False)

    def _analyze_analog_data(self):
        """Run the analog analysis pipeline assuming that the analog
        data exists.

        This calls to the external `analog_trace_runner` function to do
        the heavy lifting.
        """
        analog_data = pd.read_csv(
            self.analog_fname,
            header=None,
            names=["stimulus", "run"],
            index_col=False,
            # sep="\",  # old data format is \t
        )

        self.analog_analyzed = analog_trace_runner(
            self.metadata.fname,
            analog_data,
            self.analog,
            self.metadata,
            occluder=False,
        )

    def read_dff_of_hdf5(self):
        """Returns an np.ndarray of dF/F traces (cell x time).

        If there was a filteration step using the CaImAn GUI then the method
        will only return the filtered cells.
        """
        with h5py.File(self.results_hdf5, "r") as f:
            try:
                accepted_list = f["estimates"]["accepted_list"][()]
            except KeyError:
                accepted_list = None
            dff = f["estimates"]["F_dff"][()]
        if isinstance(accepted_list, np.ndarray):
            dff = dff[accepted_list]
        return dff

    def separate_colabeled(self):
        """Generate two arrays of DFF - colabeled and not colabeled"""
        colabeled_idx = np.load(self.colabeled)
        with h5py.File(self.results_hdf5, "r") as f:
            dff = f["estimates"]["F_dff"][()]
            colabeled = dff[
                np.intersect1d(f["estimates"]["accepted_list"][()], colabeled_idx)
            ]
            total = np.arange(len(dff))
            not_colabeled = np.setdiff1d(total, colabeled_idx)
            not_colabeled = dff[
                np.intersect1d(not_colabeled, f["estimates"]["accepted_list"])
            ]
        return colabeled, not_colabeled

    def _mock_data(self):
        """Generates a Dataset when the analog data is missing.

        When we have no analog data we can divide the experiment into
        meaningful epochs, so we have to have a special function that
        takes care of that.
        """
        data_vars = {
            "dff": (["neuron", "time"], np.atleast_2d(self.fluo_trace)),
            "epoch_times": (
                ["epoch", "time"],
                np.ones((1, self.fluo_trace.shape[1]), dtype=np.bool),
            ),
        }
        coords = {
            "neuron": np.arange(self.fluo_trace.shape[0]),
            "time": np.arange(self.fluo_trace.shape[1]) / self.metadata.fps,
            "epoch": ["spont"],
            "fov": self.metadata.fov,
            "mouse_id": self.metadata.mouse_id,
            "condition": self.metadata.condition,
            "day": self.metadata.day,
            "fname": self.metadata.fname.stem,
        }
        attrs = {"fps": self.metadata.fps, "stim_window": 1.5}
        self.fluo_analyzed = dff_dataset_init(data_vars, coords, attrs)

    def parse(self):
        """Main method to parse a single duo of analog and fluorescent data.

        After it's run, self.fluo_analyzed is populated with a Dataset containing
        the analyzed data.
        """
        if self.analog is not AnalogAcquisitionType.NONE:
            self._analyze_analog_data()
        else:
            self._mock_data()

        if self.colabeled:
            colabeled, non_colabeled = self.separate_colabeled()
            self.fluo_colabeled = self.analog_analyzed * colabeled
            self.fluo_non_colabeled = self.analog_analyzed * non_colabeled

        else:
            self.fluo_trace = self.read_dff_of_hdf5()
            try:
                if (
                    not self.fluo_trace or len(self.fluo_trace.shape) == 0
                ):  # no cells detected
                    self.fluo_trace = np.array([])
            except ValueError:
                pass
            if self.fluo_trace.shape[0] == 0:
                self.fluo_analyzed = None
                return
            self.fluo_analyzed = self.analog_analyzed * self.fluo_trace

            if self.summarize_in_plot:
                viz = SingleFovViz(self, show=True)
                viz.draw()

    def add_metadata_and_serialize(self):
        """
        Write a full DataArray to disk after parsing the FOV, if it doesn't exist yet.
        The new coordinates order is (epoch, neuron, time, mouse_id, fov, condition).
        """
        try:
            _ = next(
                pathlib.Path(self.metadata.fname).parent.glob(
                    str(self.metadata.fname.name)[:-4] + ".nc"
                )
            )
        except StopIteration:  # the file doesn't exist, we'll make a new one
            try:
                raw_data = self.fluo_analyzed.dff
            except AttributeError:  # no fluo, but perhaps we have colabeled data
                if self.colabeled:
                    self.fluo_colabeled.to_netcdf(
                        str(self.metadata.fname)[:-4] + "_colabeled.nc", mode="w"
                    )
                    self.fluo_non_colabeled.to_netcdf(
                        str(self.metadata.fname)[:-4] + "_non_colabeled.nc", mode="w"
                    )
                    return
                else:
                    print("No fluorescent data in this FOV.")
                    return
            else:
                print("Writing new NetCDF to disk.")
                self.fluo_analyzed.to_netcdf(
                    str(self.metadata.fname)[:-4] + ".nc", mode="w"
                )


@attr.s
class SingleFovViz:
    """ Visualization object that uses an existing
    SingleFovParser object as baseline to create figures showing the
    underlying data. This object is related to the functions found
    in ``dff_tools``, but is more specific to a single FOV, i.e.
    a single experiment.

    Usage:
    Main method is ``draw``, that runs all underlying viz scripts
    to generate a big figure with the data hidden inside that FOV.
    """

    fov = attr.ib(validator=instance_of(SingleFovParser))
    save = attr.ib(default=True, validator=instance_of(bool))
    axes_for_dff = attr.ib(default=14, validator=instance_of(int))
    show = attr.ib(default=False, validator=instance_of(bool))
    fig = attr.ib(init=False)
    analog_vectors = attr.ib(init=False)
    epochs_to_display = attr.ib(init=False)

    def __attrs_post_init__(self):
        self.analog_vectors = [
            np.nan_to_num(vec)
            for vec in [
                self.fov.analog_analyzed.stim_vec,
                self.fov.analog_analyzed.juxta_vec,
                self.fov.analog_analyzed.run_vec,
            ]
        ]
        if self.fov.analog_analyzed.occluder:
            self.analog_vectors.append(
                np.nan_to_num(self.fov.analog_analyzed.occluder_vec)
            )
        all_epochs = itertools.product(["stand", "run"], ["stim", "spont", "juxta"])
        self.epochs_to_display = ["_".join(epoch) for epoch in all_epochs]

    def draw(self):
        """ Main method of the class.
        Generates a summary figure containing the data inside that
        FOV """
        num_of_axes = (
            23
            if self.fov.analog is not AnalogAcquisitionType.NONE
            else self.axes_for_dff
        )
        self.fig = plt.figure(figsize=(24, 12))
        if self.fov.analog_analyzed.occluder:
            num_of_axes += 1
        gs = gridspec.GridSpec(num_of_axes, 2)
        scatter_ax = plt.subplot(gs[: self.axes_for_dff, :])
        self._scat_spikes(scatter_ax)
        scatter_ax.xaxis.tick_top()
        scatter_ax.xaxis.set_label_position("top")
        scatter_ax.spines["top"].set_visible(True)
        scatter_ax.spines["bottom"].set_visible(False)
        if self.fov.analog is not AnalogAcquisitionType.NONE:
            gen_patches, colors = self._create_rect_patches(
                self.fov.metadata.fps, self.fov.fluo_trace.shape[1]
            )
            [scatter_ax.add_artist(p) for p in gen_patches]
            cur_used_axes = self._draw_analog_plots(gs, colors)
            auc_axes = plt.subplot(gs[cur_used_axes + 1 :, 0])
            spikes_axes = plt.subplot(gs[cur_used_axes + 1 :, 1])
            self._summarize_stats_in_epochs(auc_axes, spikes_axes)
        if self.save:
            self.fig.savefig(
                str(self.fov.metadata.fname)[:-4] + "_summary.pdf",
                transparent=True,
                dpi=300,
                format="pdf",
            )
        if self.show:
            plt.show(block=False)

    def _create_rect_patches(self, fps: float, height: int):
        """ Creates plt.patches.Rectangle patches to be later added to an existing axis.
        Each type of analog data receives a specified color. """
        all_patches = []
        all_colors = plt.get_cmap("tab20b").colors
        colors: List[Tuple[float, float, float]] = [
            all_colors[7],
            all_colors[11],
            all_colors[15],
            all_colors[19],
        ]  # manually picked due to fitting colors
        vec_and_colors = [
            (vec, colors[idx]) for idx, vec in enumerate(self.analog_vectors)
        ]

        for idx, vec in enumerate(self.analog_vectors):
            diff = np.diff(np.concatenate((vec, [0])))
            starts = np.where(diff == 1)[0]
            ends = np.where(diff == -1)[0]
            # assert len(starts) == len(ends)
            for start, end in zip(starts, ends):
                all_patches.append(
                    matplotlib.patches.Rectangle(
                        (start / fps, 0),
                        width=(end - start) / fps,
                        height=height,
                        facecolor=colors[idx],
                        alpha=0.5,
                        edgecolor="None",
                    )
                )
        return all_patches, colors

    def _scat_spikes(self, ax):
        """ Plots all dF/F traces and spikes on a given axes """
        spikes = dff_tools.locate_spikes_scipy(
            self.fov.fluo_trace, fps=self.fov.metadata.fps
        )
        time_vec = np.arange(self.fov.fluo_trace.shape[1]) / self.fov.metadata.fps
        dff_tools.scatter_spikes(
            self.fov.fluo_trace, spikes, downsample_display=1, time_vec=time_vec, ax=ax
        )

    def _draw_analog_plots(self, gs: gridspec.GridSpec, colors):
        """ For each available analog data vector add its
        data to the screen """
        labels = ["Air puff", "Juxtaposed\npuff", "Run time", "CCA\nocclusion times"]

        for idx, (label, data, color) in enumerate(
            zip(labels, self.analog_vectors, colors), self.axes_for_dff
        ):
            cur_ax = plt.subplot(gs[idx, :])
            cur_ax.plot(data, color=color)
            cur_ax.set_ylabel(label, rotation=45, fontsize=8)
            cur_ax.set_xlabel("")
            cur_ax.set_xticks([])
            cur_ax.set_ylim(-0.05, 1.05)
            cur_ax.set_yticks([])
            cur_ax.spines["top"].set_visible(False)
            cur_ax.spines["bottom"].set_visible(False)
            cur_ax.spines["right"].set_visible(False)
            cur_ax.spines["left"].set_visible(False)
            cur_ax.set_frame_on(False)

        return idx

    def _summarize_stats_in_epochs(self, ax_auc: plt.Axes, ax_spikes: plt.Axes):
        """ Add axes to the main plot showing the dF/F statistics in the
        different epochs """

        df_auc = pd.DataFrame(
            # 1000 is max number of components per FOV
            np.full((1000, len(self.epochs_to_display)), np.nan),
            columns=self.epochs_to_display,
        )
        df_spikes = df_auc.copy()
        for epoch in self.epochs_to_display:
            cur_data = filter_da(self.fov.fluo_analyzed, epoch=epoch)
            if cur_data.shape[0] == 0:
                continue
            spikes = dff_tools.locate_spikes_scipy(cur_data, self.fov.metadata.fps, thresh=0.7)
            auc = dff_tools.calc_total_auc_around_spikes(
                spikes, cur_data, self.fov.metadata.fps
            )
            df_auc[epoch][: len(auc)] = auc
            spikes = dff_tools.calc_mean_spike_num(spikes, cur_data, fps=self.fov.metadata.fps)
            df_spikes[epoch][: len(spikes)] = spikes

        sns.boxenplot(data=df_auc, ax=ax_auc)
        sns.boxenplot(data=df_spikes, ax=ax_spikes)
        for ax in [ax_auc, ax_spikes]:
            ax.spines["top"].set_visible(False)
            ax.spines["right"].set_visible(False)
            ax.set_xlabel("Epoch")
        ax_auc.set_ylabel("AUC")
        ax_spikes.set_ylabel("Spikes per second")


def _filter_condition(epoch_data: xr.Dataset, condition: str) -> xr.Dataset:
    temp = epoch_data.where(epoch_data.condition == condition)
    temp["epoch_times"] = temp["epoch_times"].astype(bool)
    epoch_data = temp
    return epoch_data


def _filter_mouse_id(epoch_data: xr.Dataset, mouse_id: str) -> xr.Dataset:
    if isinstance(epoch_data.mouse_id.values, np.ndarray):
        return epoch_data
    relevant_fnames = (epoch_data.mouse_id == mouse_id).values
    return epoch_data.sel(fname=relevant_fnames)


def filter_da(
    data: xr.Dataset,
    epoch: str,
    condition: Optional[str] = None,
    mouse_id: Optional[str] = None,
) -> np.ndarray:
    """ Filter a Dataset by the given condition, mouse_id and epoch.
         Returns a numpy array in the shape of cells x time """
    epoch_data = data.sel(epoch=epoch)
    if condition:
        epoch_data = _filter_condition(epoch_data, condition)
    if mouse_id:
        epoch_data = _filter_mouse_id(epoch_data, mouse_id)

    # The final array, in the shape of (all_cells x time), will
    # be concatenated from nan-filled arrays that will be created on the fly.
    try:
        number_of_files = len(data.fname)
    except TypeError:  # unsized arrays have no len
        stacked_dff = np.full((len(data.neuron), len(data.time)), np.nan)
        relevant_epoch_dff = _generate_epoch_df(epoch_data)
        last_column = relevant_epoch_dff.shape[1]
        stacked_dff[:, :last_column] = relevant_epoch_dff
        return stacked_dff[np.isfinite(stacked_dff).any(axis=1)]

    number_of_neurons = len(data.neuron)
    stacked_dff = np.full((number_of_files * number_of_neurons, len(data.time)), np.nan)
    last_full_row = 0
    for _, dff_ds in epoch_data.groupby("fname"):
        relevant_epoch_dff = _generate_epoch_df(dff_ds)
        last_column = relevant_epoch_dff.shape[1]
        stacked_dff[
            last_full_row : (last_full_row + number_of_neurons), :last_column
        ] = relevant_epoch_dff
        last_full_row += number_of_neurons

    full_rows = np.isfinite(stacked_dff).any(axis=1)
    return stacked_dff[full_rows]


def _generate_epoch_df(ds):
    relevant_epoch_idx = ds["epoch_times"].data
    dff = ds["dff"].data
    if dff.ndim == 3:
        dff = dff[0]
    if relevant_epoch_idx.shape[0] != dff.shape[1]:
        relevant_epoch_idx = relevant_epoch_idx[0].ravel()
    relevant_epoch_dff = dff[:, relevant_epoch_idx]
    return relevant_epoch_dff


if __name__ == '__main__':
    fname = '/data/Amit_QNAP/Calcium_FXS/data_of_day_1.nc'
    data = xr.open_dataset(fname)
    for mouse_id, ds in data.groupby('mouse_id'):
        dff = filter_da(ds, 'all')
        print(dff)
        break
